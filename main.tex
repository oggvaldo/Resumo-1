\documentclass{article}

\usepackage{hyperref}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpeg}
\usepackage{float}
\usepackage{xcolor}
\usepackage[brazil]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\title{Resumo 1 - Framework de uso de ética em sistemas de IA   \\ \small Estudos em Inteligência Artificial}

\author{Anayran Pinheiro}

\begin{document}

\maketitle

\section{Resumo}

O artigo escrito por Krafft e Hauer (em colaboração com outros demais colaboradores) define um framework para a implantação e operacionalização de estruturas de ética em IA a serem usadas em projetos para uso público, em especial o europeu, visando que tais valores são crescentes e essenciais no dia a dia da sociedade. Ao verificar que o crescente uso demanda que a implantação de até onde limitar a IA para o seu uso e ainda continuar sob controle de seus criadores, uma vez que dependendo da aplicação criada ela pode se aprimorar sem interferência externa e, com isso, ferir marcos éticos. Com isso, os autores propões que baseados no modelo VCIO (em tradução livre, Valores, Critéria, Indicadores e Observações) com o uso de princípios abstratos e demonstração do funcionamento do modelo baseado nos valores de transparência, justiça, privacidade, conveniencia, esforço (em uma tradução livre de accountability) e sustentabilidade ambiental.

Ao longo o artigo, os criadores mostram que com o peso de cada um dos valores apresentados, é possível se montar em um gráfico plano é totalmente viável a criação de uma matriz de riscos, com valores de intensidade de potencial ameaças e dependências na decisão para determinados tipos de uso da IA na tomada de decisão, indo desde a escolha de uma propaganda específica de roupas para um determinado tipo de perfil (mostrando o mínimo de risco de ameaças e dependência de decisão) até a escolha de medicamentos e terapias (mostrando um risco altíssimo de ameaças e dependência de decisão). A partir desta matriz, os autores definem classificações sobre a IA, onde a IA pode ser usada sem risco de ferimento ético requerido, com a classificação 0 até a classificação 4, onde o uso de IA não pode ser usado, já que o risco de responsabilidade não pode estar em uma máquina por estar lidando com uma questão fortemente delicada na sociedade (voltando ao exemplo de escolha de um medicamento ou terapia ou direcionamento de anúncios).

Com isso, a framework é uma sugestão de como se organizar e planejar o uso de um sistema com IA para determinadas aplicações, auxiliar os criadores de sistemas com IA a definir o escopo de uso de seu sistema (com foco em quanto maior a sua classificação para o uso de IA ter também seu uso para um sistema ainda mais específico e interdependente), órgãos reguladores e a definição para a regulação ou evitar uma não regulação para uso de um sistema IA para um determinado fim, consumidores afim de compararem como tal sistema afeta o seu consumo, criação e ter um pouco de confiança regulatória para o uso e compradores, sabendo que terão um sistema transparente e reconhecido como um sistema éticamente responsável. Porém, ao mesmo tempo os autores deixam claro a necessidade de que o framework não se encontra 100% fechado, uma vez que este framework ainda necessita de refinamentos e ajustes, porém já oferece um forte e consolidado norte para o desenvolvimento da tomada de decisão de risco e uso da IA em uma determinada atividade.


\begin{thebibliography}{9}
\bibitem{"From Principles to Practice An interdisciplinary framework to operationalise AI ethics-annotated".}
Krafft, Tobias D; Hauer, Marc
\textit{Retirado de ResearchGate}
 https://www.researchgate.net/publication/340378463, , 09/09/2020
\end{thebibliography}








\end{document}
